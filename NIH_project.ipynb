{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1Cw48i_eju2vBsZpDGRUmJ7MB6pJGh5_0",
      "authorship_tag": "ABX9TyNRTvEsEdqzIhZjKojkOvl4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toinnn/Torax_Xray-Health-Care/blob/main/NIH_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLzQzBHuHA0J",
        "outputId": "11a23b98-1e41-45f9-c025-dbe919412853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Obtém o número de núcleos da CPU\n",
        "num_cores = os.cpu_count()\n",
        "\n",
        "# Obtém informações sobre a CPU\n",
        "cpu_info = !lscpu\n",
        "\n",
        "# Imprime o número de núcleos e informações sobre a CPU\n",
        "print(f\"Número de núcleos da CPU: {num_cores}\")\n",
        "# print(\"Informações sobre a CPU:\")\n",
        "# for line in cpu_info:\n",
        "#     print(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RusLtc5u4zyz",
        "outputId": "a9f46587-8870-4e76-d336-30b01a609c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de núcleos da CPU: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Git_Dir    = \"/content/drive/MyDrive/Github_Dir/Torax_Xray-Health-Care\" #https://github.com/toinnn/Torax_Xray-Health-Care\n",
        "Neural_Dir = \"Torax_Xray-Health-Care\"\n",
        "with open(\"/content/drive/MyDrive/Github_Dir/acess_Token_Git.txt\",\"r\") as file:\n",
        "    acess_Token_Git = file.read()\n",
        "Git_Path   = \"https://\"+ acess_Token_Git + \"@github.com/toinnn/\" + Neural_Dir + \".git\"\n",
        "# Git_CB_Path= \"https://\"+ acess_Token_Git + \"@github.com/toinnn/\" + \"Chat_Bot\" + \".git\""
      ],
      "metadata": {
        "id": "hGjEYWH-WQFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone \"{Git_Path}\" ./temp/Torax_Xray-Health-Care\n",
        "# !git clone \"{Git_CB_Path}\" ./temp/Chat_Bot\n",
        "\n",
        "!mv ./temp/* \"{Git_Dir}\"\n",
        "!rm -rf ./temp\n",
        "\n",
        "!rsync -aP \"{Git_Dir}\"/*  ./\n",
        "# #!ln -s \"/content/drive/MyDrive/Github_Dir/Chat_Bot\" + Neural_Dir NLP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L25_RTrWadA",
        "outputId": "b412cc53-438a-452f-9638-6acffceba71d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into './temp/Torax_Xray-Health-Care'...\n",
            "remote: Enumerating objects: 267, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (22/22), done.\u001b[K\n",
            "remote: Total 267 (delta 14), reused 23 (delta 10), pack-reused 235\u001b[K\n",
            "Receiving objects: 100% (267/267), 1.23 MiB | 21.38 MiB/s, done.\n",
            "Resolving deltas: 100% (143/143), done.\n",
            "mv: inter-device move failed: './temp/Torax_Xray-Health-Care' to '/content/drive/MyDrive/Github_Dir/Torax_Xray-Health-Care/Torax_Xray-Health-Care'; unable to remove target: Directory not empty\n",
            "sending incremental file list\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd \"/content/{Neural_Dir}\"\n",
        "# %cd \"/content/drive/MyDrive/Github_Dir/Torax_Xray-Health-Care\"\n",
        "\n",
        "# %cd /content/Chat_Bot"
      ],
      "metadata": {
        "id": "IJalNV5aW1u6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b7f423a-3424-4bbc-bcaa-b20fcb2c9b78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Torax_Xray-Health-Care\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def image_i_path(id):\n",
        "    if id <= 4999 :\n",
        "        i = 1\n",
        "    elif id <=  14999 :\n",
        "        i = 2\n",
        "    elif id <=  24999 :\n",
        "        i = 3\n",
        "    elif id <=  34999 :#24999 + 10000:\n",
        "        i = 4\n",
        "    elif id <=  44999 : #24999 + 9829:\n",
        "        i = 5\n",
        "    elif id <=  54999 : #24999 + 9829:\n",
        "        i = 6\n",
        "    elif id <=  64999 : #24999 + 9829:\n",
        "        i = 7\n",
        "    elif id <=  74999 : #24999 + 9829:\n",
        "        i = 8\n",
        "    elif id <=  84999 : #24999 + 9829:\n",
        "        i = 9\n",
        "    elif id <=  94999 : #24999 + 9829:\n",
        "        i = 10\n",
        "    elif id <= 104999 : #24999 + 9829:\n",
        "        i = 11\n",
        "    elif id <= 104999 + 7121 : #24999 + 9829:\n",
        "        i = 12\n",
        "    if i < 10 :\n",
        "      return  f\"/content/drive/MyDrive/X-ray_2/Dataset/images_00{i}/images\"\n",
        "    else :\n",
        "      return  f\"/content/drive/MyDrive/X-ray_2/Dataset/images_0{i}/images\""
      ],
      "metadata": {
        "id": "vowbpNzzV4ZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Transformer_Decoder import decoder , Trainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import DataLoader , Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import json\n",
        "# from torchvision.io.image import decode_png, read_image\n",
        "from torchvision.io import decode_png, read_image , ImageReadMode\n",
        "import torchvision.transforms as T\n",
        "import polars as pl\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_image_nvjpngl_gpu(image_path):\n",
        "    \"\"\"\n",
        "    Loads an image from the specified file path using NVJPEG decoder on GPU and returns a PyTorch tensor.\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "    Returns:\n",
        "        torch.Tensor: The PyTorch tensor representing the image.\n",
        "    \"\"\"\n",
        "    data = read_image(image_path , mode = ImageReadMode.GRAY )\n",
        "    return data.float()\n",
        "    # print(data)\n",
        "    tensor = decode_png(data).float()#.to(\"cuda\")\n",
        "    return tensor\n",
        "\n",
        "def ponderar_classes( data_ID :list , Data_Entry_path : str) -> torch.tensor:\n",
        "    Data_Entry  = pl.read_csv(Data_Entry_path)\n",
        "    list_IDs = tuple( data_ID )\n",
        "\n",
        "\n",
        "def meu_collate(batch):\n",
        "    entradas, saidas  = zip(*batch)\n",
        "    # Pad nas sequências de entrada\n",
        "    entradas_padded = pad_sequence(entradas, batch_first=True, padding_value=0)\n",
        "\n",
        "    # Pad nas sequências de saída\n",
        "    saidas_padded = pad_sequence(saidas, batch_first=True, padding_value=0)\n",
        "    # dev = torch.device(\"cpu\")\n",
        "    # if torch.cuda.is_available():\n",
        "    #     # dev = torch.device(\"cuda\")\n",
        "    #     return entradas_padded, saidas_padded , torch.cat([i.view(1,-1) for i in w ] , dim = 0 )\n",
        "\n",
        "    return entradas_padded, saidas_padded\n",
        "    # return pad_sequence([a, b, c])\n",
        "class dataset_NIH_Chest(Dataset):\n",
        "    'Characterizes a dataset for PyTorch'\n",
        "    def __init__(self, list_IDs : list[str] , Data_Entry_path : str , image_dir_path : str , label2class_path : str , max_label_lengh : int , ponderar = False ):\n",
        "        'Initialization'\n",
        "        # self.labels = labels\n",
        "        self.list_IDs = tuple(list_IDs)       #Lista contendo os nomes das imagens\n",
        "        self.image_dir_path = image_dir_path  #string com o path da pasta que contem as imagens de input\n",
        "        self.Data_Entry  = pl.read_csv(Data_Entry_path)\n",
        "        self.label2class = json.load(open(label2class_path , \"rb\"))\n",
        "\n",
        "        self.Data_Entry = {row['Image Index'] : row['Finding Labels'].split(\"|\") for row in self.Data_Entry[['Image Index' ,'Finding Labels' ]].iter_rows(named=True)}\n",
        "\n",
        "        \"\"\"for row in data_intro[['Image Index' ,'Finding Labels' ]].iter_rows(named=True) :\n",
        "            if max_label_lengh < len( row['Finding Labels'].split(\"|\")  )  : \"\"\"\n",
        "        # keys = tuple(self.Data_Entry.keys())\n",
        "        # self.name_2_index = { keys[i] : i+1  for i in range(len(keys)) }\n",
        "\n",
        "        self.class_weight = None\n",
        "        if ponderar :\n",
        "            self.class_weight = torch.zeros(len(self.label2class.keys()))\n",
        "            self.gerar_ponderacao()\n",
        "        else :\n",
        "            self.class_weight = torch.ones(len(self.label2class.keys())).float()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            self.class_weight.cuda()\n",
        "\n",
        "    def gerar_ponderacao(self):\n",
        "        for ID in self.list_IDs :\n",
        "            for i in self.Data_Entry[ID] :\n",
        "                self.class_weight[int(self.label2class[i])] += 1\n",
        "            # aux = [ int(self.label2class[i]) for i in self.Data_Entry[ID]]\n",
        "        # for i in range(len(self.class_weight)) :\n",
        "        #     self.class_weight[i] = 1/self.class_weight[i]\n",
        "        self.class_weight[0] = len(self.list_IDs)\n",
        "        self.class_weight = self.class_weight.float()**(-1)\n",
        "    def __len__(self):\n",
        "        'Denotes the total number of samples'\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "        # linha_encontrada = self.Data_Entry.filter(( self.Data_Entry['Image Index'] == self.list_IDs[index] ) )\n",
        "        # linha_encontrada['Finding Labels'].split(\"|\")\n",
        "\n",
        "        y = torch.tensor([ int(self.label2class[i]) for i in self.Data_Entry[ID]] ).view(-1, 1) #linha_encontrada['Finding Labels'][0].split(\"|\") ]).view(-1, 1)\n",
        "\n",
        "\n",
        "\n",
        "        # aux = torch.zeros( self.y_len_max - y.shape[0]  , 1)\n",
        "        aux = torch.zeros( 1  , 1)\n",
        "        y = torch.cat([y , aux] , dim = 0)\n",
        "\n",
        "        # Load data and get label\n",
        "        X = load_image_nvjpngl_gpu(self.image_dir_path + self.list_IDs[index]) #torch.load( self.image_dir_path + self.list_IDs[index] )\n",
        "\n",
        "\n",
        "        # path = image_i_path( self.name_2_index[ID] ) + \"/\" + ID\n",
        "        # X = load_image_nvjpngl_gpu(path)\n",
        "\n",
        "        # w = torch.ones(len(self.class_weight)  ).float()\n",
        "        # for i in y.view(-1):\n",
        "        #     i = int(i)\n",
        "        #     w[i] = self.class_weight[i]\n",
        "\n",
        "        return X, y\n",
        "\n",
        "\n",
        "class my_model(nn.Module):\n",
        "    def __init__(self ,  device : torch.device = torch.device(\"cpu\")) -> None:\n",
        "        super(my_model , self  ).__init__()\n",
        "        vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
        "        # vits14.eval()\n",
        "        # print(vits14.eval())\n",
        "        self.encoder = vits14.to(device) #EU NÃO LEMBRO QUANTO DEVERIA SER O MODEL_DIM !!!!!\n",
        "        self.decoder = decoder(model_dim = 768 ,heads = 8 ,num_layers = 8 , num_Classes = 16 , device = device)\n",
        "        self.device  = device\n",
        "        # self.transform_image = T.Compose([T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])] )\n",
        "        self.transform_image = T.Compose([T.Resize(224),  T.Normalize([0.5], [0.5])] )\n",
        "\n",
        "    def setDevice(self , device : torch.device) :\n",
        "        self.encoder = self.encoder.to( device )\n",
        "        self.decoder.setDevice( device )\n",
        "        return\n",
        "\n",
        "\n",
        "    def forward_fit(self, image  , max_lengh = 100):\n",
        "        # print(f\"Passou do Encoder image : { image.shape}\")\n",
        "        img2 = []\n",
        "        for img in image :\n",
        "            img = img.view(1, img.shape[0] , img.shape[1])\n",
        "            img = torch.cat( [img,img,img] , dim = 0 )\n",
        "            # print(f\"img.shape = {img.shape}\")\n",
        "            # print(f\"trasnform : {self.transform_image( img  )[:3].unsqueeze(0).shape }\")\n",
        "            img2 += [self.transform_image( img )[:3].unsqueeze(0) ]\n",
        "        # image = [ self.encoder(self.transform_image( img.view(1 , img.shape[0] , img.shape[1]))[:3].unsqueeze(0) ).view(1,1,-1)     for img in image]\n",
        "        image = [ self.encoder(img).view(1 , 1 , -1) for img in img2 ]\n",
        "        enc   = torch.cat(image , dim = 0 )\n",
        "        # image = self.transform_image(image)[:3].unsqueeze(0)\n",
        "        # print(f\"Passou do Encoder image : {image.shape}\")\n",
        "        # enc = self.encoder(image)\n",
        "        # print(f\"Passou do Encoder enc : {enc.shape}\")\n",
        "        return self.decoder.forward_fit(enc , enc , max_lengh)\n",
        "\n",
        "    def forward(self, image  , max_lengh = 100):\n",
        "        # image = self.transform_image(image)[:3].unsqueeze(0)\n",
        "        # enc = self.encoder(image)\n",
        "\n",
        "        img2 = []\n",
        "        for img in image :\n",
        "            img = img.view(1, img.shape[0] , img.shape[1])\n",
        "            img = torch.cat( [img,img,img] , dim = 0 )\n",
        "            # print(f\"img.shape = {img.shape}\")\n",
        "            # print(f\"trasnform : {self.transform_image( img  )[:3].unsqueeze(0).shape }\")\n",
        "            img2 += [self.transform_image( img )[:3].unsqueeze(0) ]\n",
        "        # image = [ self.encoder(self.transform_image( img.view(1 , img.shape[0] , img.shape[1]))[:3].unsqueeze(0) ).view(1,1,-1)     for img in image]\n",
        "        image = [ self.encoder(img).view(1 , 1 , -1) for img in img2 ]\n",
        "        enc   = torch.cat(image , dim = 0 )\n",
        "\n",
        "        return self.decoder(enc , enc , max_lengh)\n",
        "\n",
        "class my_model_medium(nn.Module):\n",
        "    def __init__(self ,  device : torch.device = torch.device(\"cpu\")) -> None:\n",
        "        super(my_model_medium , self  ).__init__()\n",
        "        vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
        "        # vits14.eval()\n",
        "        # print(vits14.eval())\n",
        "        self.encoder = vits14.to(device) #EU NÃO LEMBRO QUANTO DEVERIA SER O MODEL_DIM !!!!!\n",
        "        self.decoder = decoder(model_dim = 768 ,heads = 4 ,num_layers = 5 , num_Classes = 16 , device = device)\n",
        "        self.device  = device\n",
        "        # self.transform_image = T.Compose([T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])] )\n",
        "        self.transform_image = T.Compose([T.Resize(224),  T.Normalize([0.5], [0.5])] )\n",
        "\n",
        "    def setDevice(self , device : torch.device) :\n",
        "        self.encoder = self.encoder.to( device )\n",
        "        self.decoder.setDevice( device )\n",
        "        return\n",
        "\n",
        "\n",
        "    def forward_fit(self, image  , max_lengh = 100):\n",
        "        # print(f\"Passou do Encoder image : { image.shape}\")\n",
        "        img2 = []\n",
        "        for img in image :\n",
        "            img = img.view(1, img.shape[0] , img.shape[1])\n",
        "            img = torch.cat( [img,img,img] , dim = 0 )\n",
        "            # print(f\"img.shape = {img.shape}\")\n",
        "            # print(f\"trasnform : {self.transform_image( img  )[:3].unsqueeze(0).shape }\")\n",
        "            img2 += [self.transform_image( img )[:3].unsqueeze(0) ]\n",
        "        # image = [ self.encoder(self.transform_image( img.view(1 , img.shape[0] , img.shape[1]))[:3].unsqueeze(0) ).view(1,1,-1)     for img in image]\n",
        "        image = [ self.encoder(img).view(1 , 1 , -1) for img in img2 ]\n",
        "        enc   = torch.cat(image , dim = 0 )\n",
        "        # image = self.transform_image(image)[:3].unsqueeze(0)\n",
        "        # print(f\"Passou do Encoder image : {image.shape}\")\n",
        "        # enc = self.encoder(image)\n",
        "        # print(f\"Passou do Encoder enc : {enc.shape}\")\n",
        "        return self.decoder.forward_fit(enc , enc , max_lengh)\n",
        "\n",
        "    def forward(self, image  , max_lengh = 100):\n",
        "        # image = self.transform_image(image)[:3].unsqueeze(0)\n",
        "        # enc = self.encoder(image)\n",
        "\n",
        "        img2 = []\n",
        "        for img in image :\n",
        "            img = img.view(1, img.shape[0] , img.shape[1])\n",
        "            img = torch.cat( [img,img,img] , dim = 0 )\n",
        "            # print(f\"img.shape = {img.shape}\")\n",
        "            # print(f\"trasnform : {self.transform_image( img  )[:3].unsqueeze(0).shape }\")\n",
        "            img2 += [self.transform_image( img )[:3].unsqueeze(0) ]\n",
        "        # image = [ self.encoder(self.transform_image( img.view(1 , img.shape[0] , img.shape[1]))[:3].unsqueeze(0) ).view(1,1,-1)     for img in image]\n",
        "        image = [ self.encoder(img).view(1 , 1 , -1) for img in img2 ]\n",
        "        enc   = torch.cat(image , dim = 0 )\n",
        "\n",
        "        return self.decoder(enc , enc , max_lengh)\n",
        "\n",
        "class my_model_small(nn.Module):\n",
        "    def __init__(self ,  device : torch.device = torch.device(\"cpu\")) -> None:\n",
        "        super(my_model_small , self  ).__init__()\n",
        "        vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')\n",
        "        # vits14.eval()\n",
        "        # print(vits14.eval())\n",
        "        self.encoder = vits14.to(device) #EU NÃO LEMBRO QUANTO DEVERIA SER O MODEL_DIM !!!!!\n",
        "        self.decoder = decoder(model_dim = 768 ,heads = 3 ,num_layers = 3 , num_Classes = 16 , device = device)\n",
        "        self.device  = device\n",
        "        # self.transform_image = T.Compose([T.Resize(244), T.CenterCrop(224), T.Normalize([0.5], [0.5])] )\n",
        "        self.transform_image = T.Compose([T.Resize(224),  T.Normalize([0.5], [0.5])] )\n",
        "\n",
        "    def setDevice(self , device : torch.device) :\n",
        "        self.encoder = self.encoder.to( device )\n",
        "        self.decoder.setDevice( device )\n",
        "        return\n",
        "\n",
        "\n",
        "    def forward_fit(self, image  , max_lengh = 100):\n",
        "        # print(f\"Passou do Encoder image : { image.shape}\")\n",
        "        img2 = []\n",
        "        for img in image :\n",
        "            img = img.view(1, img.shape[0] , img.shape[1])\n",
        "            img = torch.cat( [img,img,img] , dim = 0 )\n",
        "            # print(f\"img.shape = {img.shape}\")\n",
        "            # print(f\"trasnform : {self.transform_image( img  )[:3].unsqueeze(0).shape }\")\n",
        "            img2 += [self.transform_image( img )[:3].unsqueeze(0) ]\n",
        "        # image = [ self.encoder(self.transform_image( img.view(1 , img.shape[0] , img.shape[1]))[:3].unsqueeze(0) ).view(1,1,-1)     for img in image]\n",
        "        image = [ self.encoder(img).view(1 , 1 , -1) for img in img2 ]\n",
        "        enc   = torch.cat(image , dim = 0 )\n",
        "        # image = self.transform_image(image)[:3].unsqueeze(0)\n",
        "        # print(f\"Passou do Encoder image : {image.shape}\")\n",
        "        # enc = self.encoder(image)\n",
        "        # print(f\"Passou do Encoder enc : {enc.shape}\")\n",
        "        return self.decoder.forward_fit(enc , enc , max_lengh)\n",
        "\n",
        "    def forward(self, image  , max_lengh = 100):\n",
        "        # image = self.transform_image(image)[:3].unsqueeze(0)\n",
        "        # enc = self.encoder(image)\n",
        "\n",
        "        img2 = []\n",
        "        for img in image :\n",
        "            img = img.view(1, img.shape[0] , img.shape[1])\n",
        "            img = torch.cat( [img,img,img] , dim = 0 )\n",
        "            # print(f\"img.shape = {img.shape}\")\n",
        "            # print(f\"trasnform : {self.transform_image( img  )[:3].unsqueeze(0).shape }\")\n",
        "            img2 += [self.transform_image( img )[:3].unsqueeze(0) ]\n",
        "        # image = [ self.encoder(self.transform_image( img.view(1 , img.shape[0] , img.shape[1]))[:3].unsqueeze(0) ).view(1,1,-1)     for img in image]\n",
        "        image = [ self.encoder(img).view(1 , 1 , -1) for img in img2 ]\n",
        "        enc   = torch.cat(image , dim = 0 )\n",
        "\n",
        "        return self.decoder(enc , enc , max_lengh)\n"
      ],
      "metadata": {
        "id": "oiJQ5ozfmOu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    max_label_lengh = 9\n",
        "    params = {'batch_size': 25   ,\n",
        "        'shuffle'         : True ,\n",
        "        'num_workers'     : 8    ,\n",
        "        \"collate_fn\"      : meu_collate}\n",
        "\n",
        "    path_project  = \"/content/drive/Othercomputers/Meu laptop/Dataset_work_space/Torax_Xray\"\n",
        "    image_dir_path = path_project + \"/images/\"\n",
        "    data_intro_path = path_project + \"/Data_Entry_2017.csv\"\n",
        "    label2class_path = path_project + \"/target_dict_label2class.json\"\n",
        "\n",
        "\n",
        "\n",
        "    train_path = path_project + \"/train_val_list.txt\"\n",
        "    test_path  = path_project + \"/test_list.txt\"\n",
        "\n",
        "    train_id = [i.replace(\"\\n\" , \"\") for i in open(train_path , \"r\").readlines() ][:330]\n",
        "    test_id  = [i.replace(\"\\n\" , \"\") for i in open(test_path , \"r\" ).readlines() ][:350]\n",
        "\n",
        "    training_set = dataset_NIH_Chest(train_id , data_intro_path , image_dir_path  , label2class_path , max_label_lengh )\n",
        "    dataset_weights = training_set.class_weight\n",
        "    training_loader = DataLoader(training_set  , **params)\n",
        "\n",
        "    test_set = dataset_NIH_Chest(test_id , data_intro_path , image_dir_path  , label2class_path , max_label_lengh )\n",
        "    test_loader = DataLoader(test_set, **params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # model   = my_model(torch.device(\"cuda\"))\n",
        "    model   = my_model_medium(torch.device(\"cuda\"))\n",
        "    model   = my_model_small(torch.device(\"cuda\"))\n",
        "\n",
        "    trainer = Trainer(model , torch.device(\"cuda\") )\n",
        "    # model , loss  = trainer.fit(training_loader  , 0.05 , 1 , 1 , test_dataloader = test_loader , model_class  = my_model , class_weight = dataset_weights.cuda())\n",
        "    # model , loss  = trainer.fit(training_loader  , 0.05 , 1 , 1 , test_dataloader = test_loader , model_class  = my_model_medium , class_weight = dataset_weights.cuda())\n",
        "    model , loss  = trainer.fit(training_loader  , 0.0005 , 1 , 3 , test_dataloader = test_loader , model_class  = my_model_small , class_weight = dataset_weights.cuda())\n",
        "\n",
        "    path_2_counter = \"/content/drive/MyDrive/Github_Dir/models_saved.json\"\n",
        "\n",
        "    with open(path_2_counter , \"r\") as arquivo_json:\n",
        "        counter = json.load(arquivo_json)\n",
        "    new_id = int(counter[\"ID_atual\"])\n",
        "\n",
        "    path_2_save = f\"/content/drive/MyDrive/Github_Dir/xRay_model_{new_id}_loss_{loss}.model\"\n",
        "\n",
        "    try:\n",
        "        with open(path_2_save, \"wb\") as arquivo:\n",
        "            pickle.dump(model, arquivo)\n",
        "    except FileNotFoundError:\n",
        "        with open(path_2_save, \"xb\") as arquivo:\n",
        "            pickle.dump(model, arquivo)\n",
        "\n",
        "    counter[\"ID_atual\"] = f\"{new_id + 1}\"\n",
        "\n",
        "    with open(path_2_counter , \"w\") as arquivo_json:\n",
        "        json.dump(counter , arquivo_json)\n"
      ],
      "metadata": {
        "id": "ecDQjXlifOEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbdc6ca-9a40-440d-820e-2ca2ca8912b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "Age atual 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ctd atual 0  samples processados 0\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.027994168922305107\n",
            "Pré backward\n",
            "Pós backward\n",
            " ctd atual 1  samples processados 25\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.02214619517326355\n",
            "Pré backward\n",
            "Pós backward\n",
            "lossTestList : [0.03444761904761905]\n",
            "bestLossValue = inf\n",
            "Novo melhor\n",
            "Objeto salvo em best_model_in_test.pickle\n",
            "Saiu do Melhor\n",
            " ctd atual 2  samples processados 50\n",
            "out.shape = torch.Size([25, 16, 5]) , y.shape = torch.Size([25, 5])\n",
            "loss : 0.01717291958630085\n",
            "Pré backward\n",
            "Pós backward\n",
            " ctd atual 3  samples processados 75\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.02244608849287033\n",
            "Pré backward\n",
            "Pós backward\n",
            "lossTestList : [0.03444761904761905, 0.03472571428571428]\n",
            "bestLossValue = 0.03444761904761905\n",
            " ctd atual 4  samples processados 100\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.021946074441075325\n",
            "Pré backward\n",
            "Pós backward\n",
            " ctd atual 5  samples processados 125\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.021646060049533844\n",
            "Pré backward\n",
            "Pós backward\n",
            "lossTestList : [0.03444761904761905, 0.03472571428571428, 0.034420952380952385]\n",
            "bestLossValue = 0.03444761904761905\n",
            "Novo melhor\n",
            "Objeto salvo em best_model_in_test.pickle\n",
            "Saiu do Melhor\n",
            " ctd atual 6  samples processados 150\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.021846042945981026\n",
            "Pré backward\n",
            "Pós backward\n",
            " ctd atual 7  samples processados 175\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.021846039220690727\n",
            "Pré backward\n",
            "Pós backward\n",
            "lossTestList : [0.03444761904761905, 0.03472571428571428, 0.034420952380952385, 0.03448761904761906]\n",
            "bestLossValue = 0.034420952380952385\n",
            " ctd atual 8  samples processados 200\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.02204602211713791\n",
            "Pré backward\n",
            "Pós backward\n",
            " ctd atual 9  samples processados 225\n",
            "out.shape = torch.Size([25, 16, 6]) , y.shape = torch.Size([25, 6])\n",
            "loss : 0.01427512988448143\n",
            "Pré backward\n",
            "Pós backward\n",
            "lossTestList : [0.03444761904761905, 0.03472571428571428, 0.034420952380952385, 0.03448761904761906, 0.034588571428571434]\n",
            "bestLossValue = 0.034420952380952385\n",
            " ctd atual 10  samples processados 250\n",
            "out.shape = torch.Size([25, 16, 3]) , y.shape = torch.Size([25, 3])\n",
            "loss : 0.03032800741493702\n",
            "Pré backward\n",
            "Pós backward\n",
            " ctd atual 11  samples processados 275\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.02184600941836834\n",
            "Pré backward\n",
            "Pós backward\n",
            "lossTestList : [0.03444761904761905, 0.03472571428571428, 0.034420952380952385, 0.03448761904761906, 0.034588571428571434, 0.034779047619047626]\n",
            "bestLossValue = 0.034420952380952385\n",
            " ctd atual 12  samples processados 300\n",
            "out.shape = torch.Size([25, 16, 4]) , y.shape = torch.Size([25, 4])\n",
            "loss : 0.022146005183458328\n",
            "Pré backward\n",
            "Pós backward\n",
            " ctd atual 13  samples processados 65\n",
            "out.shape = torch.Size([5, 16, 2]) , y.shape = torch.Size([5, 2])\n",
            "loss : 0.23745982348918915\n",
            "Pré backward\n",
            "Pós backward\n",
            "lossTestList : [0.03444761904761905, 0.03472571428571428, 0.034420952380952385, 0.03448761904761906, 0.034588571428571434, 0.034779047619047626, 0.034531428571428574]\n",
            "bestLossValue = 0.034420952380952385\n",
            "lossTestList : [0.03444761904761905, 0.03472571428571428, 0.034420952380952385, 0.03448761904761906, 0.034588571428571434, 0.034779047619047626, 0.034531428571428574, 0.034653333333333335]\n",
            "bestLossValue = 0.034420952380952385\n",
            "O melhor resultado de teste foi  0.034420952380952385\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/facebookresearch_dinov2_main\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n",
            "chegou aqui\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_image_nvjpngl_gpu('/content/drive/Othercomputers/Meu laptop/Dataset_work_space/Torax_Xray/images/00005762_004.png')\n",
        "# load_image_pil_accelerated('/content/drive/Othercomputers/Meu laptop/Dataset_work_space/Torax_Xray/images/00005762_004.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx980fiIY82R",
        "outputId": "2f971b78-0cec-468b-f579-c284e064d0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
              "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
              "         [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
              "         ...,\n",
              "         [ 97., 163., 152.,  ...,  44.,  46.,  26.],\n",
              "         [106., 179., 165.,  ...,  47.,  49.,  27.],\n",
              "         [ 53.,  89.,  81.,  ...,  23.,  23.,  13.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_id)\n",
        "{\"my_model_medium\":[\"xray_model_(5 e 6)\"] ,\"my_model\":[\"xray_model_(0 ao 4)\" , \"my_model_small\":[\"xray_model_(7,8)\"]]}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfXdPqCTgsyh",
        "outputId": "a3147097-2e00-4fd8-93ad-d50f37844129"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25596"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vits14 = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitb14')"
      ],
      "metadata": {
        "id": "E2u6NuL80DlG",
        "outputId": "2da9dce2-c046-4544-baa2-6a50d848fa83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
            "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
            "  warnings.warn(\"xFormers is not available (Attention)\")\n",
            "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
            "  warnings.warn(\"xFormers is not available (Block)\")\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitb14/dinov2_vitb14_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitb14_pretrain.pth\n",
            "100%|██████████| 330M/330M [00:02<00:00, 169MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "path = \"/content/xRay_model_1_loss_0.03098666666666666.model\"\n",
        "with open(path, 'rb') as file:\n",
        "    # model = pickle.load(file)\n",
        "    model = pickle.load(file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "hixJRzkWyFrj",
        "outputId": "e5381631-a3cb-4175-e9d4-4e83e8129443"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-acedb904f3d5>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# model = pickle.load(file)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_load_from_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUNSAFE_MESSAGE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1254\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1256\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1191\u001b[0m                 \u001b[0;31m# stop wrapping with TypedStorage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m                 typed_storage = torch.storage.TypedStorage(\n\u001b[0;32m-> 1193\u001b[0;31m                     \u001b[0mwrap_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                     _internal=True)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cuda_deserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_cuda_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_torch_load_uninitialized\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         raise RuntimeError('Attempting to deserialize object on a CUDA '\n\u001b[0m\u001b[1;32m    259\u001b[0m                            \u001b[0;34m'device but torch.cuda.is_available() is False. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                            \u001b[0;34m'If you are running on a CPU-only machine, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install datasets\n"
      ],
      "metadata": {
        "id": "qs59mC7iD6JA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# # Carregando o dataset \"alkzar90/NIH-Chest-X-ray-dataset\"\n",
        "# dataset = load_dataset(\"alkzar90/NIH-Chest-X-ray-dataset\", 'image-classification' )\n",
        "\n",
        "# # Acessando os dados de treinamento\n",
        "# train_data = dataset[\"train\"]\n",
        "\n",
        "# # Imprimindo algumas informações\n",
        "# print(f\"Número de exemplos no conjunto de treinamento: {len(train_data)}\")\n",
        "# print(f\"Exemplo de texto: {train_data[0]['text']}\")\n",
        "# print(f\"Rótulo do exemplo: {train_data[0]['label']}\")\n"
      ],
      "metadata": {
        "id": "MEe866skEAwU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}